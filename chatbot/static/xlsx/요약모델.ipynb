{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 파일이 로컬에 없습니다. 다운로드를 시작합니다...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1d3b3017714482a1321e9364d5c4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ggml-model-Q4_K_M.gguf:   0%|          | 0.00/6.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brian\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\brian\\.cache\\huggingface\\hub\\models--heegyu--EEVE-Korean-Instruct-10.8B-v1.0-GGUF. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 435 tensors from C:\\Users\\brian\\.cache\\huggingface\\hub\\models--heegyu--EEVE-Korean-Instruct-10.8B-v1.0-GGUF\\snapshots\\9bf4892cf2017362dbadf99bd9a3523387135362\\ggml-model-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 48\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,40960]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,40960]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,40960]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   97 tensors\n",
      "llama_model_loader: - type q4_K:  289 tensors\n",
      "llama_model_loader: - type q6_K:   49 tensors\n",
      "llm_load_vocab: special tokens cache size = 4\n",
      "llm_load_vocab: token to piece cache size = 0.2243 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 40960\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 48\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 34B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 10.80 B\n",
      "llm_load_print_meta: model size       = 6.06 GiB (4.82 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.20 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 다운로드되었습니다: C:\\Users\\brian\\.cache\\huggingface\\hub\\models--heegyu--EEVE-Korean-Instruct-10.8B-v1.0-GGUF\\snapshots\\9bf4892cf2017362dbadf99bd9a3523387135362\\ggml-model-Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_tensors:        CPU buffer size =  6210.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.16 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1542\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '48', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\\n' + system_message + '<|im_end|>\\n'}}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\n",
      "' + system_message + '<|im_end|>\n",
      "'}}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <s>\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'finalDAta.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m\n\u001b[0;32m     22\u001b[0m llm \u001b[38;5;241m=\u001b[39m Llama(\n\u001b[0;32m     23\u001b[0m     model_path\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[0;32m     24\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# CPU 코어 수\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     n_ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m,  \u001b[38;5;66;03m# 컨텍스트 윈도우\u001b[39;00m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Excel 파일 읽기 (처음 3개의 행만)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinalDAta.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 프롬프트 템플릿 정의\u001b[39;00m\n\u001b[0;32m     34\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m다음은 그림에 대한 정보입니다:\u001b[39m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[38;5;124m제목: \u001b[39m\u001b[38;5;132;01m{title}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124m객관적 설명:\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\brian\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(\n\u001b[0;32m    496\u001b[0m         io,\n\u001b[0;32m    497\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    498\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    499\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\brian\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1551\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m   1552\u001b[0m     )\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\brian\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1403\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\brian\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'finalDAta.xlsx'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "import time\n",
    "\n",
    "# 모델 정보\n",
    "model_name_or_path = \"heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF\"\n",
    "model_basename = \"ggml-model-Q4_K_M.gguf\"\n",
    "\n",
    "# 모델 파일 경로\n",
    "model_path = os.path.join(os.getcwd(), model_basename)\n",
    "\n",
    "# 모델 다운로드 (필요한 경우에만)\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"모델 파일이 로컬에 없습니다. 다운로드를 시작합니다...\")\n",
    "    model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "    print(f\"모델이 다운로드되었습니다: {model_path}\")\n",
    "else:\n",
    "    print(f\"모델 파일이 이미 존재합니다: {model_path}\")\n",
    "# GPU 사용 설정 (코랩 환경에 맞게 조정)\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2,  # CPU 코어 수\n",
    "    n_batch=512,  # 1과 n_ctx 사이여야 하며, GPU VRAM 양을 고려해야 합니다.\n",
    "    n_gpu_layers=43,  # 모델과 GPU VRAM에 따라 이 값을 조정하세요.\n",
    "    n_ctx=4096,  # 컨텍스트 윈도우\n",
    ")\n",
    "\n",
    "# Excel 파일 읽기 (처음 3개의 행만)\n",
    "df = pd.read_excel('finalDAta.xlsx', nrows=3)\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "template = \"\"\"다음은 그림에 대한 정보입니다:\n",
    "\n",
    "제목: {title}\n",
    "평론: {description}\n",
    "유형: {type}\n",
    "상세 설명: {description_plus}\n",
    "\n",
    "위의 정보를 바탕으로, 그림에 대한 객관적이고 간결한 설명을 100단어 이내로 작성해주세요.\n",
    "그림의 주요 소재, 스타일, 그리고 전반적인 분위기에 초점을 맞춰주세요.\n",
    "\n",
    "객관적 설명:\n",
    "\"\"\"\n",
    "\n",
    "# 각 행에 대해 설명 생성\n",
    "def generate_description(row):\n",
    "    prompt = template.format(\n",
    "        title=row['TITLE'],\n",
    "        description=row['DESCRIPTION'],\n",
    "        type=row['TYPE'],\n",
    "        description_plus=row['description_plus']\n",
    "    )\n",
    "    response = llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        stop=['Human:', '\\n\\n'],\n",
    "        echo=False\n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "# 새로운 설명 생성 및 저장\n",
    "df['description_final'] = df.apply(generate_description, axis=1)\n",
    "\n",
    "# 결과를 Excel 파일로 저장\n",
    "df.to_excel('tokenized_semart_test_combined_with_final_description.xlsx', index=False)\n",
    "\n",
    "print(\"처리가 완료되었습니다. 결과가 새 Excel 파일에 저장되었습니다.\")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n처리된 데이터:\")\n",
    "print(df[['DESCRIPTION', 'TYPE', 'description_plus', 'description_final']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing data: 'dict' object has no attribute 'validate_fields'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brian\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class RestaurantData(BaseModel):\n",
    "    개설일자: str = Field(description=\"Date of establishment in YYYY-MM-DD format\")\n",
    "    주소: str = Field(description=\"Address in 제주시 XX동 format\")\n",
    "    가맹점명_포함_텍스트: str = Field(description=\"Text containing store name\")\n",
    "    이용_건수_상위: int = Field(description=\"Usage count percentile\", le=100, ge=0)\n",
    "    총_매출_상위: int = Field(description=\"Total sales percentile\", le=100, ge=0)\n",
    "    건당_이용_금액_상위: int = Field(description=\"Amount per usage percentile\", le=100, ge=0)\n",
    "    여행_요일: str = Field(description=\"Preferred days of travel (comma-separated)\")\n",
    "    성별_선호: str = Field(description=\"Gender preference (남 or 여)\")\n",
    "    선호_나이대: str = Field(description=\"Preferred age group (20,30,40,50,60)\")\n",
    "    현지인맛집: bool = Field(description=\"Whether it's a local favorite\")\n",
    "    분류: str = Field(description=\"Restaurant category (카페, 한식, 중식, 단품요리)\")\n",
    "    오름차순: str = Field(description=\"Ascending order feature\")\n",
    "\n",
    "    class Config:\n",
    "        validate_assignment = True\n",
    "\n",
    "    @classmethod\n",
    "    def validate_date_format(cls, value: str) -> bool:\n",
    "        try:\n",
    "            datetime.strptime(value, '%Y-%m-%d')\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    @classmethod\n",
    "    def validate_days(cls, value: str) -> bool:\n",
    "        valid_days = {'월', '화', '수', '목', '금', '토', '일'}\n",
    "        days = {day.strip() for day in value.split(',')}\n",
    "        return all(day in valid_days for day in days)\n",
    "\n",
    "    def validate_fields(self) -> List[str]:\n",
    "        errors = []\n",
    "        \n",
    "        # Validate date format\n",
    "        if not self.validate_date_format(self.개설일자):\n",
    "            errors.append(\"Invalid date format. Use YYYY-MM-DD\")\n",
    "            \n",
    "        # Validate address format\n",
    "        if not self.주소.startswith(\"제주시\"):\n",
    "            errors.append(\"Address must start with '제주시'\")\n",
    "            \n",
    "        # Validate percentiles\n",
    "        for field, value in {\n",
    "            \"이용_건수_상위\": self.이용_건수_상위,\n",
    "            \"총_매출_상위\": self.총_매출_상위,\n",
    "            \"건당_이용_금액_상위\": self.건당_이용_금액_상위\n",
    "        }.items():\n",
    "            if not (0 <= value <= 100):\n",
    "                errors.append(f\"{field} must be between 0 and 100\")\n",
    "                \n",
    "        # Validate days\n",
    "        if not self.validate_days(self.여행_요일):\n",
    "            errors.append(\"Invalid day format\")\n",
    "            \n",
    "        # Validate gender\n",
    "        if self.성별_선호 not in [\"남\", \"여\"]:\n",
    "            errors.append(\"Gender must be either '남' or '여'\")\n",
    "            \n",
    "        # Validate age group\n",
    "        if self.선호_나이대 not in [\"20\", \"30\", \"40\", \"50\", \"60\"]:\n",
    "            errors.append(\"Invalid age group\")\n",
    "            \n",
    "        # Validate category\n",
    "        if self.분류 not in [\"카페\", \"한식\", \"중식\", \"단품요리\"]:\n",
    "            errors.append(\"Invalid category\")\n",
    "            \n",
    "        return errors\n",
    "\n",
    "def create_restaurant_parser():\n",
    "    \"\"\"Create a JSON parser for restaurant data\"\"\"\n",
    "    return JsonOutputParser(pydantic_object=RestaurantData)\n",
    "\n",
    "def process_restaurant_data(text_output: str) -> Optional[RestaurantData]:\n",
    "    \"\"\"Process the text output and return structured restaurant data\"\"\"\n",
    "    try:\n",
    "        parser = create_restaurant_parser()\n",
    "        parsed_data = parser.parse(text_output)\n",
    "        \n",
    "        # Validate the parsed data\n",
    "        validation_errors = parsed_data.validate_fields()\n",
    "        if validation_errors:\n",
    "            print(\"Validation errors:\", validation_errors)\n",
    "            return None\n",
    "            \n",
    "        return parsed_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    sample_output = '''\n",
    "    {\n",
    "        \"개설일자\": \"2023-01-15\",\n",
    "        \"주소\": \"제주시 노형동\",\n",
    "        \"가맹점명_포함_텍스트\": \"제주돌담식당\",\n",
    "        \"이용_건수_상위\": 85,\n",
    "        \"총_매출_상위\": 75,\n",
    "        \"건당_이용_금액_상위\": 65,\n",
    "        \"여행_요일\": \"월,화,일\",\n",
    "        \"성별_선호\": \"여\",\n",
    "        \"선호_나이대\": \"30\",\n",
    "        \"현지인맛집\": true,\n",
    "        \"분류\": \"한식\",\n",
    "        \"오름차순\": \"성별_선호\"\n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    result = process_restaurant_data(sample_output)\n",
    "    if result:\n",
    "        print(\"Successfully parsed restaurant data:\", result.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.1.tar.gz (63.9 MB)\n",
      "     ---------------------------------------- 0.0/63.9 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/63.9 MB 435.7 kB/s eta 0:02:27\n",
      "     --------------------------------------- 0.1/63.9 MB 980.4 kB/s eta 0:01:06\n",
      "     ---------------------------------------- 0.5/63.9 MB 3.7 MB/s eta 0:00:18\n",
      "      --------------------------------------- 1.1/63.9 MB 5.8 MB/s eta 0:00:11\n",
      "      --------------------------------------- 1.5/63.9 MB 6.5 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 1.9/63.9 MB 6.9 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 2.4/63.9 MB 7.2 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 2.8/63.9 MB 7.5 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 3.4/63.9 MB 8.0 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 3.9/63.9 MB 8.4 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 4.5/63.9 MB 8.7 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 5.0/63.9 MB 8.9 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 5.6/63.9 MB 9.1 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 6.1/63.9 MB 9.3 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 6.6/63.9 MB 9.4 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 7.2/63.9 MB 9.6 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 7.8/63.9 MB 9.7 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 8.3/63.9 MB 9.8 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 8.8/63.9 MB 9.9 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 9.2/63.9 MB 9.8 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 9.8/63.9 MB 10.0 MB/s eta 0:00:06\n",
      "     ------ -------------------------------- 10.4/63.9 MB 10.9 MB/s eta 0:00:05\n",
      "     ------ -------------------------------- 10.9/63.9 MB 10.9 MB/s eta 0:00:05\n",
      "     ------ -------------------------------- 11.5/63.9 MB 11.1 MB/s eta 0:00:05\n",
      "     ------- ------------------------------- 12.0/63.9 MB 11.3 MB/s eta 0:00:05\n",
      "     ------- ------------------------------- 12.5/63.9 MB 11.3 MB/s eta 0:00:05\n",
      "     ------- ------------------------------- 13.1/63.9 MB 11.5 MB/s eta 0:00:05\n",
      "     -------- ------------------------------ 13.6/63.9 MB 11.5 MB/s eta 0:00:05\n",
      "     -------- ------------------------------ 14.1/63.9 MB 11.5 MB/s eta 0:00:05\n",
      "     -------- ------------------------------ 14.7/63.9 MB 11.5 MB/s eta 0:00:05\n",
      "     --------- ----------------------------- 15.2/63.9 MB 11.5 MB/s eta 0:00:05\n",
      "     --------- ----------------------------- 15.7/63.9 MB 11.7 MB/s eta 0:00:05\n",
      "     --------- ----------------------------- 15.9/63.9 MB 11.5 MB/s eta 0:00:05\n",
      "     --------- ----------------------------- 16.0/63.9 MB 10.7 MB/s eta 0:00:05\n",
      "     --------- ----------------------------- 16.2/63.9 MB 10.4 MB/s eta 0:00:05\n",
      "     --------- ----------------------------- 16.3/63.9 MB 10.1 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 16.7/63.9 MB 9.8 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 17.2/63.9 MB 9.8 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 17.7/63.9 MB 9.8 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 18.3/63.9 MB 9.8 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 18.8/63.9 MB 9.8 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 19.4/63.9 MB 9.9 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 19.9/63.9 MB 9.9 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 20.5/63.9 MB 9.9 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 21.0/63.9 MB 9.9 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 21.6/63.9 MB 9.9 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 22.1/63.9 MB 9.9 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 22.7/63.9 MB 9.9 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 23.2/63.9 MB 9.9 MB/s eta 0:00:05\n",
      "     -------------- ------------------------ 23.8/63.9 MB 10.1 MB/s eta 0:00:04\n",
      "     -------------- ------------------------ 24.3/63.9 MB 10.1 MB/s eta 0:00:04\n",
      "     --------------- ----------------------- 24.9/63.9 MB 10.1 MB/s eta 0:00:04\n",
      "     --------------- ----------------------- 25.4/63.9 MB 10.1 MB/s eta 0:00:04\n",
      "     --------------- ----------------------- 26.0/63.9 MB 10.1 MB/s eta 0:00:04\n",
      "     ---------------- ---------------------- 26.5/63.9 MB 11.5 MB/s eta 0:00:04\n",
      "     ---------------- ---------------------- 27.1/63.9 MB 11.7 MB/s eta 0:00:04\n",
      "     ---------------- ---------------------- 27.7/63.9 MB 11.7 MB/s eta 0:00:04\n",
      "     ----------------- --------------------- 28.2/63.9 MB 11.9 MB/s eta 0:00:03\n",
      "     ----------------- --------------------- 28.8/63.9 MB 11.9 MB/s eta 0:00:03\n",
      "     ----------------- --------------------- 29.3/63.9 MB 11.9 MB/s eta 0:00:03\n",
      "     ------------------ -------------------- 29.9/63.9 MB 11.9 MB/s eta 0:00:03\n",
      "     ------------------ -------------------- 30.4/63.9 MB 11.9 MB/s eta 0:00:03\n",
      "     ------------------ -------------------- 31.0/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------- ------------------- 31.5/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------- ------------------- 32.1/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------- ------------------- 32.6/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     -------------------- ------------------ 33.2/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     -------------------- ------------------ 33.8/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     -------------------- ------------------ 34.2/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     --------------------- ----------------- 34.8/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     --------------------- ----------------- 35.4/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     --------------------- ----------------- 35.9/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     ---------------------- ---------------- 36.5/63.9 MB 11.9 MB/s eta 0:00:03\n",
      "     ---------------------- ---------------- 37.0/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     ---------------------- ---------------- 37.6/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     ----------------------- --------------- 38.2/63.9 MB 11.7 MB/s eta 0:00:03\n",
      "     ----------------------- --------------- 38.7/63.9 MB 11.9 MB/s eta 0:00:03\n",
      "     ----------------------- --------------- 39.2/63.9 MB 11.9 MB/s eta 0:00:03\n",
      "     ------------------------ -------------- 39.8/63.9 MB 11.9 MB/s eta 0:00:03\n",
      "     ------------------------ -------------- 40.3/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ------------------------ -------------- 40.9/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ------------------------- ------------- 41.4/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ------------------------- ------------- 42.0/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ------------------------- ------------- 42.6/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 43.1/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 43.7/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 44.2/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 44.8/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 45.3/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 45.9/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 46.4/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 47.0/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 47.5/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ----------------------------- --------- 48.0/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ----------------------------- --------- 48.6/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ------------------------------ -------- 49.2/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ------------------------------ -------- 49.7/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ------------------------------ -------- 50.3/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ------------------------------- ------- 50.8/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ------------------------------- ------- 51.4/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     ------------------------------- ------- 51.9/63.9 MB 11.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------ 52.5/63.9 MB 11.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 53.0/63.9 MB 11.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 53.5/63.9 MB 11.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 54.0/63.9 MB 11.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 54.6/63.9 MB 11.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 54.7/63.9 MB 11.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 54.8/63.9 MB 10.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 54.9/63.9 MB 10.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 55.1/63.9 MB 10.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 55.4/63.9 MB 9.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 56.0/63.9 MB 9.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 56.6/63.9 MB 9.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 57.1/63.9 MB 9.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 57.7/63.9 MB 9.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 58.2/63.9 MB 9.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 58.8/63.9 MB 9.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 59.3/63.9 MB 9.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 59.8/63.9 MB 9.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 60.4/63.9 MB 9.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 60.9/63.9 MB 9.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 61.5/63.9 MB 9.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 62.0/63.9 MB 9.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  62.6/63.9 MB 9.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.1/63.9 MB 9.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.7/63.9 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.9/63.9 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 63.9/63.9 MB 9.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from llama-cpp-python) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.5/45.5 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [20 lines of output]\n",
      "      \u001b[32m*** \u001b[1mscikit-build-core 0.10.7\u001b[0m using \u001b[34mCMake 3.30.5\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "      \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "      2024-10-25 08:35:01,097 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "      loading initial cache file C:\\Users\\brian\\AppData\\Local\\Temp\\tmpnwresmb_\\build\\CMakeInit.txt\n",
      "      -- Building for: NMake Makefiles\n",
      "      CMake Error at CMakeLists.txt:3 (project):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n",
      "      CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      \u001b[31m\n",
      "      \u001b[1m***\u001b[0m \u001b[31mCMake configuration failed\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_model\u001b[39m(model_name_or_path, model_basename, model_dir):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "import time\n",
    "\n",
    "def download_model(model_name_or_path, model_basename, model_dir):\n",
    "    \"\"\"모델 다운로드 함수\"\"\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, model_basename)\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"모델 파일이 로컬에 없습니다. 다운로드를 시작합니다...\")\n",
    "        downloaded_path = hf_hub_download(\n",
    "            repo_id=model_name_or_path,\n",
    "            filename=model_basename,\n",
    "            local_dir=model_dir\n",
    "        )\n",
    "        print(f\"모델이 다운로드되었습니다: {downloaded_path}\")\n",
    "        return downloaded_path\n",
    "    else:\n",
    "        print(f\"모델 파일이 이미 존재합니다: {model_path}\")\n",
    "        return model_path\n",
    "\n",
    "def initialize_llm(model_path):\n",
    "    \"\"\"LLM 초기화 함수\"\"\"\n",
    "    try:\n",
    "        return Llama(\n",
    "            model_path=model_path,\n",
    "            n_threads=os.cpu_count(),  # 시스템의 CPU 코어 수에 맞게 자동 설정\n",
    "            n_batch=512,\n",
    "            n_gpu_layers=43,\n",
    "            n_ctx=4096,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"LLM 초기화 중 오류 발생: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def generate_description(row, llm, template):\n",
    "    \"\"\"개별 행에 대한 설명 생성 함수\"\"\"\n",
    "    try:\n",
    "        prompt = template.format(\n",
    "            title=str(row['TITLE']),\n",
    "            description=str(row['DESCRIPTION']),\n",
    "            type=str(row['TYPE']),\n",
    "            description_plus=str(row['description_plus'])\n",
    "        )\n",
    "        \n",
    "        response = llm(\n",
    "            prompt=prompt,\n",
    "            max_tokens=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            stop=['Human:', '\\n\\n'],\n",
    "            echo=False\n",
    "        )\n",
    "        return response['choices'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"행 처리 중 오류 발생: {str(e)}\")\n",
    "        return f\"오류 발생: {str(e)}\"\n",
    "\n",
    "def main():\n",
    "    # 설정\n",
    "    model_name_or_path = \"heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF\"\n",
    "    model_basename = \"ggml-model-Q4_K_M.gguf\"\n",
    "    model_dir = \"C:/model\"\n",
    "    \n",
    "    # 프롬프트 템플릿\n",
    "    template = \"\"\"다음은 그림에 대한 정보입니다:\n",
    "\n",
    "제목: {title}\n",
    "평론: {description}\n",
    "유형: {type}\n",
    "상세 설명: {description_plus}\n",
    "\n",
    "위의 정보를 바탕으로, 그림에 대한 객관적이고 간결한 설명을 100단어 이내로 작성해주세요.\n",
    "그림의 주요 소재, 스타일, 그리고 전반적인 분위기에 초점을 맞춰주세요.\n",
    "\n",
    "객관적 설명:\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # 모델 다운로드\n",
    "        model_path = download_model(model_name_or_path, model_basename, model_dir)\n",
    "        \n",
    "        # LLM 초기화\n",
    "        llm = initialize_llm(model_path)\n",
    "        \n",
    "        # Excel 파일 읽기\n",
    "        print(\"Excel 파일을 읽는 중...\")\n",
    "        df = pd.read_excel('finalDAta.xlsx', nrows=3)\n",
    "        \n",
    "        # 설명 생성\n",
    "        print(\"설명을 생성하는 중...\")\n",
    "        descriptions = []\n",
    "        for idx, row in df.iterrows():\n",
    "            print(f\"행 {idx + 1} 처리 중...\")\n",
    "            description = generate_description(row, llm, template)\n",
    "            descriptions.append(description)\n",
    "            time.sleep(1)  # API 요청 간 간격\n",
    "        \n",
    "        df['description_final'] = descriptions\n",
    "        \n",
    "        # 결과 저장\n",
    "        output_file = 'tokenized_semart_test_combined_with_final_description.xlsx'\n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"\\n처리가 완료되었습니다. 결과가 {output_file}에 저장되었습니다.\")\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(\"\\n처리된 데이터:\")\n",
    "        print(df[['DESCRIPTION', 'TYPE', 'description_plus', 'description_final']])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"프로그램 실행 중 오류 발생: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.1.tar.gz (63.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from llama-cpp-python) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.1-cp312-cp312-win_amd64.whl size=3208621 sha256=63ba8604806eef5999766f311eaeb8b51e48a4492f1e2e482fe3941cefe77065\n",
      "  Stored in directory: c:\\users\\brian\\appdata\\local\\pip\\cache\\wheels\\58\\d4\\b6\\f219a2c6af82353b2a21923250728c3d180c95a5ad9ec1c6c3\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.1\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
